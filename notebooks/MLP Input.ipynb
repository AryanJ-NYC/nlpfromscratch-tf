{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialization feature parameters.\n",
    "word_dim = 2 # embedding dimension\n",
    "feat_dim = 1 # embedding dimension\n",
    "num_feats = 2\n",
    "win_len = 1\n",
    "vocab_size = 5 \n",
    "feat_size = 10 \n",
    "seq_len = win_len * 2 + 1 # Total number of tokens that will be represented in the input\n",
    "\n",
    "# For clarity make sure that values of vectors are largely different\n",
    "# Word Embeddings are in (0,1) , while Feature Embeddings are ints in (1,feat_size) \n",
    "word_embs_init = np.array([ [i / 10.] * word_dim for i in range(vocab_size) ])\n",
    "feat_embs_init = np.array([ [i] * feat_dim for i in range(vocab_size, vocab_size + feat_size) ]) # do shifting for easy viewing\n",
    "\n",
    "# Total Input Dimension\n",
    "input_dim = ( seq_len  ) * ( word_dim  + (num_feats * feat_dim)) # num tokens times dense vector dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all words dense vectore representation\n",
      "[[ 0.   0. ]\n",
      " [ 0.1  0.1]\n",
      " [ 0.2  0.2]\n",
      " [ 0.3  0.3]\n",
      " [ 0.4  0.4]]\n",
      "all feature dense vectore representation\n",
      "[[ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]]\n"
     ]
    }
   ],
   "source": [
    "# Now the word embedding values are different from feature imbedding values for \n",
    "print 'all words dense vectore representation'\n",
    "print word_embs_init\n",
    "print 'all feature dense vectore representation'\n",
    "print feat_embs_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Word IDs\n",
      "[[3 4 4]]\n",
      "Input Feature IDs\n",
      "[[[9 6]\n",
      "  [6 6]\n",
      "  [8 1]]]\n"
     ]
    }
   ],
   "source": [
    "# Generate Random Integers as Token IDs for input\n",
    "batch_size = 1\n",
    "input_words = np.random.randint(0, vocab_size, (batch_size, seq_len) )\n",
    "input_feats = np.random.randint(0, feat_size , (batch_size, seq_len, num_feats) )\n",
    "\n",
    "print 'Input Word IDs'\n",
    "print input_words\n",
    "print 'Input Feature IDs'\n",
    "print input_feats \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vectors\n",
      "[[[ 0.30000001  0.30000001]\n",
      "  [ 0.40000001  0.40000001]\n",
      "  [ 0.40000001  0.40000001]]]\n",
      "feat vectors\n",
      "[[[[ 14.]\n",
      "   [ 11.]]\n",
      "\n",
      "  [[ 11.]\n",
      "   [ 11.]]\n",
      "\n",
      "  [[ 13.]\n",
      "   [  6.]]]]\n",
      "reshape features\n",
      "[[[ 9  9  9  9  9 11 11 11 11 11]\n",
      "  [13 13 13 13 13 14 14 14 14 14]\n",
      "  [12 12 12 12 12  9  9  9  9  9]]]\n",
      "input seq after concatenation - Batched Input Vectors\n",
      "[[  0.30000001   0.30000001  14.          11.           0.40000001\n",
      "    0.40000001  11.          11.           0.40000001   0.40000001  13.\n",
      "    6.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create the input sequence by looking up dense vector represenation of words and features, \n",
    "# then concaten\n",
    "# Build Lookup Layers\n",
    "word_emb = tf.Variable(word_embs_init, dtype=tf.float32)\n",
    "feat_emb = tf.Variable(feat_embs_init, dtype=tf.float32)\n",
    "\n",
    "# Convert int token ids to dense vectors via lookup table\n",
    "word_lookup = tf.nn.embedding_lookup(word_emb, input_words)\n",
    "feat_lookup = tf.nn.embedding_lookup(feat_emb, input_feats)\n",
    "\n",
    "# Concatnate dense vectors into single input vector (batched)\n",
    "# first reshape feature vectors, concating along last dimension\n",
    "feat_flat = tf.reshape(feat_lookup, (batch_size, seq_len, -1))\n",
    "word_feats = tf.concat([word_lookup, feat_flat], axis=2)\n",
    "input_seq = tf.reshape(word_feats, [batch_size, -1])\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Now we can see the token / feature id to embedding lookup\n",
    "print 'word vectors'\n",
    "print sess.run(word_lookup)\n",
    "print 'feat vectors'\n",
    "print sess.run(feat_lookup)\n",
    "print 'reshape features'\n",
    "print sess.run(feat_reshaped)\n",
    "print 'input seq after concatenation - Batched Input Vectors'\n",
    "print sess.run(input_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
